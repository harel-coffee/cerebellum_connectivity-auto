{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling Cortico-Cerebellar Connectivity\n",
    "\n",
    "## Questions:\n",
    "* What is the best model of cortico-cerebellar connectivity? \n",
    "* What is the topography of connections between the cortex and the cerebellum?\n",
    "    * One-to-one connection topography?\n",
    "\n",
    "## Goals:\n",
    "* Estimate a model on cortical activity patterns and predict cerebellar activity pattern on a new set of data.\n",
    "* Find the most useful model of cortico-cerebellar connectivity\n",
    "\n",
    "## Background:\n",
    "### Cortico-cerebellar connectivity in non-human primates\n",
    "* (Kelly & Strick, 2003)\n",
    "\n",
    "### Cortico-cerebellar connectivity in humans\n",
    "* Methods using resting state functional connectivity:\n",
    "    * (Habas et al., 2009)\n",
    "    * (Buckner et al., 2011)\n",
    "    * (Marek et al., 2018)\n",
    "* Methods using functional gradients:\n",
    "    * (Tian et al., 2020)\n",
    "    * (Guell et al., 2018)\n",
    "\n",
    "### Gap:\n",
    "* assuming one to one connections, the previous models of cortico-cerebellar connectivity in humans, have used a winner-take-all approach and discovered the functional organization of cerebellum. __How am I going to address this using PLS or bi-clustering?__\n",
    "* cortico-cerebellar networks not assuming the one-to-one connection (with no assumptions at al)\n",
    "    \n",
    "### Hypothesis:\n",
    "* relaxing the assumption of one-to-one connection, a connectivity model that incorporates the connection topography/organization between the two structures will perform better at prediction activity patterns of the cerebellum on new dataset. _might be good if I use some materials from introduction in the bi-clustering paper_\n",
    "\n",
    "## Material and Methods:\n",
    "* MDTB dataset:\n",
    "    * two task sets, 4 sessions, 32 runs in total\n",
    "* Cortical activity pattterns were used as predictors and cerebellar activity patterns were used as responses.\n",
    "\n",
    "Re-iterating the __goal__ here: __estimate connectivity weights and connectivity pattern between the cerebellum and cortex that makes the best predictions for cerebellar activity__\n",
    "\n",
    "** How can I investigate connection topography??\n",
    "\n",
    "### Analysis:\n",
    "Multivariate multiple regression will be used to model cerebellar activity profiles as a linear function of cortical activity profiles across a range of conditions! The regression model here is:\n",
    "\\begin{align*}\n",
    "&Y_{n*m}\\ = X_{n*p}W_{p*m} + E_{n*m}\n",
    "\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Where $n$ is the number of conditions, $m$ is the number of cerebellar voxels, $p$ is the number of cortical voxels/tessels, $W$ is the matrix of connectivity weights, and $E$ is the matrix containing the residuals. $Y$ is the matrix where each column contains the activity profile of a voxel in the cerebellum (response matrix) and $X$ is the matrix where each column contains activity profile of a voxel in the neocortex (explanatory matrix, design matrix). \n",
    "\n",
    "### Limitations:\n",
    "There is high multicollinearity between explanatory variables (activity profiles of cerebellar voxels) which results from correlations between activity profiles of cortical tessles and the fact that the number of observations (in this case the number of conditions) is lower than the number of explanatory variables. This multicollinearity makes the estimated regression coefficients highly variable and will lead to poor predictions.\n",
    "\n",
    "### Regression models:\n",
    "There are a number of approaches that try to overcome this limitation. These approaches are:\n",
    "#### Step-wise Regression\n",
    "Selecting a subset of explanatory variables through a stepwise regression. This subset selection method ultimately leads to a subset of explanatory variables that best explain the variance in the response variable. This method is time-consuming for the purpose of this analysis. However, it might be a good approach to find the overlaps between cerebellar targets of cortical regions. Basically, for each cerebellar voxel, we try to find the best set of explanatory variables (cortical tessels) that best predict the activity profile, create a map of prediction accuracy of models and investigate the overlap between different models (different subsets of tassels' activity profiles)\n",
    "#### Ridge Regression\n",
    "a shrinkage method that shrinks the regression coefficients by imposing a size penalty. \n",
    "* Ridge regression and PCA: \n",
    "    * Ridge regression shrinks the direction of small variances in the design matrix the most,\n",
    "* Ridge regression as an optimization problem: It tries to optimize penalized sums of squares:\n",
    "\\begin{align*}\n",
    "&J\\ = \\sum (y_i - W_0 + \\sum x_{ij} W_j)^2 + \\lambda \\sum W_j^2\n",
    "\\\\\n",
    "\\end{align*}\n",
    "In matrix notation:\n",
    "\\begin{align*}\n",
    "&J\\ = (Y - XW)^T (Y - XW) + W^T W\n",
    "\\\\\n",
    "\\end{align*}\n",
    "    * $\\lambda$ is a parameter that determines the amount of shrinkage of the parameters. The higher the , more shrinkage will be applied\n",
    "    \n",
    "#### Principal Component Regression (PCR)\n",
    "This method tries to find a set of orthogonal variables that best explain the covariance between the explanatory variables by applying PCA to the design matrix X. Then it regresses Y on the set of these orthogonal variables. It is a two-step process and as a optimization problem: \n",
    "1. PCA to X and \n",
    "    * $X = ZV$ subject to constraint $VV^T = 1$\n",
    "        - $Z$ contains score variables and $V$ is the matrix of weights that transforms $X$ into the latent space.\n",
    "        - The objective function for this step is the objective function of PCA:\n",
    "        \\begin{align*}\n",
    "        &J\\ = |X - ZV^T|^2 = |X - XVV^T|^2\n",
    "        \\\\\n",
    "        \\end{align*}\n",
    "2. Regress $Y$ onto principal components of $X$\n",
    "    * $Y = ZW + E$ with the objective function:\n",
    "    \\begin{align*}\n",
    "    &J\\ = |Y - ZW|^2 \n",
    "    \\\\\n",
    "    \\end{align*}\n",
    "    \n",
    "Or we combine these steps together in a single step (this will give different results):\n",
    "J = Y - ZW2 = Y - XVW2, subject to constraint: VTV= 1\n",
    "\\begin{align*}\n",
    "&J\\ = |Y - ZW|^2 = |Y - XVW|^2, \\text{subject to constraint:}\\ VV^T = 1\n",
    "\\\\\n",
    "\\end{align*}\n",
    "\n",
    "As the variables are orthogonal, the problem of multicollinearity is solved. However, like all the other methods that use PCA, a question remains: What is the best number of components? In addition to this, the components are chosen to maximize the explained variance of X, not Y. Hence, the chosen components might not be good candidates to explain the highest variance in Y. To overcome the later limitation of this method, partial least squares regression (PLS-R) is used.\n",
    "\n",
    "#### Partial Least Squares Regression (PLS-R)\n",
    "This method also tries to solve the problem of multicollinearity in the design matrix. It estimates the latent space for both $X$ and $Y$ so as to maximize the covariance between the two. Basically, it tries to project both $X$ and $Y$ onto latent spaces and it tries to maximize the covariance between the latent spaces of $X$ and $Y$.\n",
    "\n",
    "\\begin{align*}\n",
    "&X_{n*p} = Z_{n*q} V_{p*q}^T + E_{n*p}\n",
    "\\\\\n",
    "\\\\\n",
    "&Y_{n*m} = U_{n*q} Q_{m*q}^T + F_{n*m}\n",
    "\\\\\n",
    "\\end{align*}\n",
    "Where $n$ is the number of conditions, $m$ is the number of cerebellar voxels, $p$ is the number of cortical voxels/tessels and $q$ is the number of PLS components.\n",
    "As an optimization problem:\n",
    "\n",
    "* As an optimization problem :\n",
    "    \\begin{align*}\n",
    "    &Z_{n*q} = X_{n*p}V_{p*q}, \\text{subject to constraint:}\\ VV^T = 1\n",
    "    \\\\\n",
    "    \\end{align*}\n",
    "    \n",
    "    $V$ is the matrix of weights that transforms $X$ into a latent space ($Z$)\n",
    "    \n",
    "    \\begin{align*}\n",
    "    &U_{n*q} = Y_{n*m}C_{m*q}, \\text{subject to constraint:}\\ CC^T = 1\n",
    "    \\\\\n",
    "    \\end{align*}\n",
    "    \n",
    "    $C$ is the matrix of weights that transforms $Y$ into a latent space ($U$)\n",
    "    \n",
    "    The objective is to find the latent variables (spaces) between which the covariance is maximized and at the same time the method tries to predict $Y$:\n",
    "    \n",
    "    \\begin{align*}\n",
    "    &cov(Z, U) = Z^TU = V^TX^TYC\n",
    "    \\\\\n",
    "    \\end{align*}\n",
    "    \n",
    "    \n",
    "* There are different implementations of the PLSR, each with different assumptions. There are both __iterative__ and __non-iterative__ methods. To my understanding the iterative method is the most common one. But there is also a non-iterative method. The difference between the results of iterative and non-iterative methods is in the __orthogonality__ of final latent variables. Briefly, using the iterative methods like __PLS1__ (for 1-D response variable), __PLS2__, and __SIMPLS__, the latent variables will be mutually orthogonal as these methods are __iterative__ and in the end of each iteration, the effect of each estimated latent variable is subtracted from the original matrices, a process called __deflation__. The deflation process guarantees mutual orthogonality of latent variables. There is, however, a non-iterative approach. The latent variables estimated using this method will not, in general, be mutually orthogonal. In any case, there is a eigenvalue problem. The iterative methods try to solve the eigenvalue problem iteratively and the non-iterative method tries to solve this eigenvalue problem in one go.\n",
    "* The algorithm tries to find $T$ and $U$ __iteratively__ and then calculates the regression coefficients as $b_i= z_i^T u_i$ at each iteration of the algorithm. Eventually, we will have a diagonal matrix ($B$) where each element is calculated in each iteration. Finally:\n",
    "\n",
    "    \\begin{align*}\n",
    "    &Y_{n*m} = X_{n*p}W_{pls}\n",
    "    \\\\\n",
    "    \\end{align*}\n",
    "    \n",
    "    where $W_{pls}= V_{p*q}^T + B_{q*q} C_{q*m}^T$. Based on this formulation, $W_{pls}$ is a $p*m$ matrix\n",
    "    \n",
    "    \n",
    "* _I need to clarify something here to better understand how the dimensions of the matrices are determined:_\n",
    "    For $A_{p*q}$, The pseudoinverse of $A$ is defined as $A_{q*p}^+$. This is useful in determining the dimensions of matrix $V$ and $W_{pls}$.\n",
    "    \n",
    "* _Keep in mind that we have $V_{p*q}$, $V_{q*p}^T$, and based on the relationship between a matrix and its pseudoinverse: $V_{p*q}^{T+}$_ \n",
    "* $B_{q*q}$ is a diagonal matrix.\n",
    "    * __Hypothesis__: _In this model, $B$ being diagonal implies that the one-to-one topography is valid. However, not restricting B to be diagonal will imply that the one-to-one topography does not necessarily hold. My hypothesis is that not restricting B to be a diagonal matrix, in other words not imposing the one-to-one connection topography, will yield better predictions. __What would this mean in terms of mathematical implementation of the algorithm to estimate the parameters and latent structures?___\n",
    "    \n",
    "#### Ridge regression + Convex bi-clustering\n",
    "This is the naive method mentioned in (Yu et al., 2019). You can estimate the parameters first (using ridge regression for example) and then apply a bi-clustering algorithm to the parameter matrix that is estimated to reveal the grouping structure in the parameter matrix.\n",
    "\n",
    "#### Simultaneous Parameter Learning and Bi-clustering for Multi-Response Models (Yu et al., 2019)\n",
    "They are interested in accurate parameter estimation as well as understanding the bi-cluster or checkerboard structure of the parameter matrix. The regression model is stated as before:\n",
    "\\begin{align*}\n",
    "&Y_{n*m}\\ = X_{n*p}W_{p*m} + E_{n*m}\n",
    "\\\\\n",
    "\\end{align*}\n",
    "And they wish to discover the bi-cluster structure among features (inputs) and responses (outputs).\n",
    "\n",
    "As an optimization problem in general: \n",
    "* the general loss function (squared loss): $L(X, Y, W) = \\sum_{s = 1}^k\\|Y_s - X_s W_s\\|_{2}^2$\n",
    "* For regularization, $l_1$ or $l_2$ norm is used\n",
    "\n",
    "What makes this method __special__? it all goes back to the loss function! Two loss functions are defined:\n",
    "###### Hard Fusion\n",
    "\\begin{align*}\n",
    "&\\min_{W} L(X, Y, W) + \\lambda _{1} R(W) + \\lambda _{2}[\\Omega _ \\omega(W) + \\Omega_ {\\tilde{\\omega}}(W^T)]\n",
    "\\\\\n",
    "\\end{align*}\n",
    "\n",
    "which can be re-written as:\n",
    "\\begin{align*}\n",
    "&\\min_{W} \\| Y - XW \\|_{F}^2 + \\lambda _{1} \\sum_{i = 1}^k \\| W_{.i} \\|_{1}+ \\lambda _{2}[\\Omega _ \\omega(W) + \\Omega_ {\\tilde{\\omega}}(W^T)]\n",
    "\\\\\n",
    "\\end{align*}\n",
    "\n",
    "###### Soft Fusion\n",
    "\\begin{align*}\n",
    "&\\min_{W, \\Gamma} \\| Y - XW \\|_{F}^2 + \\lambda _{1} \\sum_{i = 1}^k \\| W_{.i} \\|_{1}+ \\lambda_{2} \\sum_{i = 1}^k \\|W_{.i} - \\Gamma_{.i}\\|_{2}^2+\\lambda _{3}[\\Omega _ \\omega(W) + \\Omega_ {\\tilde{\\omega}}(W^T)]\n",
    "\\\\\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "\n",
    "## Results:\n",
    "Preliminary results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation\n",
    "#### Cortico-cerebellar connectivity at the subject level\n",
    "1. training model using one task set (for the current result, sc1 was used for training)\n",
    "    * __crossed training__: X data of one session was paired with Y data from another session. This cross-training method is used to tackle the problem stated in Buckner et al. (that the cortical areas adjacent to cerebellum seem to be having high correlations with the cerebellm, masking the correlations from other regions) \n",
    "    * condition subset selection: for the current results, all the conditions (including instructions) were used!\n",
    "2. testing: for testing, dataset from a task set different from the one used in training was used (for the current results, sc2 was used for testing)\n",
    "    * __crossed testing__: the procedure of creating the crossed dataset is as before (pairing Xs and Ys from different sessions).\n",
    "    * Rcv (cross validated R), Rnc (non-cross-validated R), and Rp (reliability of predictions) were used for evaluation\n",
    "\n",
    "#### Cortico-cerebellar connectivity at the group level\n",
    "Different ways of calculating group average:\n",
    "1. Average over subjects after within session average is calculated for each subject separately\n",
    "    * Load in activity patterns (both for cortex and the cerebellum) for each subject.\n",
    "    * calculate average across runs within a session for each subject\n",
    "    * average over subjects\n",
    "    * fit the model using group average data\n",
    "    * evaluate the model using group average data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages needed to visualize the results\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import evaluation  # user-defined package\n",
    "import os\n",
    "\n",
    "import glob\n",
    "import nibabel as nib\n",
    "\n",
    "from nilearn import plotting\n",
    "from nilearn import surface\n",
    "from nilearn import datasets\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ipywidgets as widgets       # interactive display\n",
    "%config InlineBackend.figure_format = 'svg' # other available formats are: 'retina', 'png', 'jpeg', 'pdf'\n",
    "# plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/nma.mplstyle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To visualize the results:\n",
    "# sestting defaults for some variables\n",
    "returnSubjs = np.array([2,3,4,6,8,9,10,12,14,15,17,18,19,20,21,22,24,25,26,27,28,29,30,31])\n",
    "# Setting different options of the functions\n",
    "roi_dict1        = {'cortex':'tesselsWB162', 'cerebellum':'grey_nan'} \n",
    "roi_dict2        = {'cortex':'tesselsWB362', 'cerebellum':'grey_nan'} \n",
    "roi_dict3        = {'cortex':'tesselsWB642', 'cerebellum':'grey_nan'} \n",
    "trainExperiment  = 1\n",
    "tMode            = 'crossed'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataframes to visualize results of evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a dataframe with all the evaluation info for subject level modelling\n",
    "# DF = evaluation.eval_df(sn = returnSubjs, glm = 7, models = ['l2regress', 'plsregress'], \n",
    "#                  rois = ['tesselsWB162', 'tesselsWB362', 'tesselsWB642'], \n",
    "#                  testExper = 2)\n",
    "\n",
    "# # save the dataframe for visualizations\n",
    "# DF.to_csv('df_subjects.csv')\n",
    "\n",
    "# # Create a dataframe with all the evaluation info for group level modelling\n",
    "# DF_group = evaluation.eval_df(sn = 'group', glm = 7, models = ['l2regress', 'plsregress'], \n",
    "#                  rois = ['tesselsWB162', 'tesselsWB362', 'tesselsWB642'], \n",
    "#                  testExper = 2)\n",
    "\n",
    "# # save the dataframe for visualizations\n",
    "# DF_group.to_csv('df_group.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluation results visualization #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "interactive(children=(Dropdown(description='level', options=('group', 'subject'), value='group'), Dropdown(des…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "93f06a98d1594d1ab55c4446379d4339"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "# interactive visualization of the results\n",
    "@widgets.interact(level = ['group', 'subject'],\n",
    "                  model = ['plsregress', 'l2regress'],\n",
    "                  xname = ['162', '362', '642'], \n",
    "                  yvar  = ['Rcv', 'Rnc', 'Rp'])\n",
    "def plot_model_params(level, model, xname, yvar):\n",
    "    # get the corresponding dataframe\n",
    "    if level == 'group':\n",
    "        myDF = pd.read_csv('df_group.csv')\n",
    "    elif level == 'subject':\n",
    "        myDF = pd.read_csv('df_subjects.csv')\n",
    "        \n",
    "    # gets the section of the dataframe corresponding to the model\n",
    "    dfx = myDF.loc[myDF.xname == 'tesselsWB%s'%xname]\n",
    "    dfx_model = dfx.loc[dfx.model == model]\n",
    "    \n",
    "    # sort the dataframe based on values of params\n",
    "    df2 = dfx_model.sort_values(by ='params' , ascending=False)\n",
    "    \n",
    "    # plot\n",
    "    figure, axes = plt. subplots(nrows=1, ncols=2, figsize = (15, 5))\n",
    "    plt.subplot(121)\n",
    "    sns.barplot(x='model', y=yvar, hue='params', data=df2)\n",
    "    plt.title('barplot of %s - %s - %s level'% (yvar, model, level))\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    sns.lineplot(x='params', y=yvar, marker='o', err_style = 'band', \n",
    "                 dashes = False, data=df2)\n",
    "    plt.title('lineplot of %s - %s - %s level'% (yvar, model, level))\n",
    "    figure. tight_layout(pad=3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluation results visualization #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "interactive(children=(Dropdown(description='level', options=('group', 'subject'), value='group'), Dropdown(des…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f7a1a1f9e906486f82f56bf5cc1f8f17"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "# interactive visualization of the results\n",
    "@widgets.interact(level = ['group', 'subject'],\n",
    "                  xname = ['162', '362', '642'], \n",
    "                  yvar  = ['Rcv', 'Rnc', 'Rp', 'Ry'])\n",
    "def plot_model_params(level, xname, yvar):\n",
    "    model = ['plsregress', 'l2regress']\n",
    "    # get the corresponding dataframe\n",
    "    if level == 'group':\n",
    "        myDF = pd.read_csv('df_group.csv')\n",
    "    elif level == 'subject':\n",
    "        myDF = pd.read_csv('df_subjects.csv')\n",
    "        \n",
    "    # gets the section of the dataframe corresponding to the model\n",
    "    dfx = myDF.loc[myDF.xname == 'tesselsWB%s'%xname]\n",
    "\n",
    "    dfx_model0 = dfx.loc[dfx.model == model[0]]\n",
    "    dfx_model1 = dfx.loc[dfx.model == model[1]]\n",
    "    \n",
    "    # convert to log space for l2regress\n",
    "#     vals = np.log10(dfx_model1['params'].values)\n",
    "#     dfx_model1['log_params'] = vals\n",
    "#     display(dfx_model1)\n",
    "    \n",
    "    # sort the dataframe based on values of params\n",
    "    df20 = dfx_model0.sort_values(by ='params' , ascending=False)\n",
    "    df21 = dfx_model1.sort_values(by ='params' , ascending=False)\n",
    "        \n",
    "    ymin = min(np.min(df20[yvar]), np.min(df21[yvar]))\n",
    "    ymax = max(np.max(df20[yvar]), np.max(df21[yvar]))\n",
    "    \n",
    "    \n",
    "    # plot\n",
    "    figure, axes = plt.subplots(nrows=1, ncols=2, figsize = (10, 5))\n",
    "    plt.subplot(121)\n",
    "    sns.lineplot(x='params', y=yvar, marker='o', err_style = 'band',\n",
    "                 dashes = False, data=df20)\n",
    "    # control y limits\n",
    "    plt.ylim(ymin - 0.01, ymax+0.01)\n",
    "    # set title\n",
    "    plt.title('%s VS parameter - %s - %s level'%(yvar, model[0], level))\n",
    "\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    sns.lineplot(x='params', y=yvar, marker='o', err_style = 'band', \n",
    "                 dashes = False, data=df21) # is it using std?????\n",
    "    # control y limits\n",
    "    plt.ylim(ymin-0.01, ymax+0.01)\n",
    "    # set title\n",
    "    plt.title('%s VS parameter - %s - %s level'%(yvar, model[1], level))\n",
    "    \n",
    "    figure. tight_layout(pad=3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cortical and Cerebellar maps\n",
    "Here, I will be showing the cortical and cerebellar maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Component loading maps for PLS regression with 7 PLS components\n",
    "gifti files for each component loadings averaged across subjects are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.image as mpimg \n",
    "# import matplotlib.pyplot as plt \n",
    "\n",
    "\n",
    "# !wget -qO 'gg_642_6_R.png' https://drive.google.com/drive/folders/1r2VSjmFGw9MUtzEZmTrYmWOghRDq_MHd/\n",
    "\n",
    "# # importing matplotlib modules \n",
    "# import matplotlib.image as mpimg \n",
    "# import matplotlib.pyplot as plt \n",
    "  \n",
    "# # Read Images \n",
    "# # plot\n",
    "# figure, axes = plt.subplots(nrows=2, ncols=2, figsize = (10, 5))\n",
    "# plt.subplot(121)\n",
    "# img = mpimg.imread(os.path.join('./preliminaryIMs/PLS', 'gg_162_0_L.png'))\n",
    "\n",
    "\n",
    "# # Output Images \n",
    "# plt.imshow(img)\n",
    "\n",
    "# plt.subplot(122)\n",
    "# img = mpimg.imread(os.path.join('./preliminaryIMs/PLS', 'gg_162_0_R.png'))\n",
    "\n",
    "# plt.subplot(211)\n",
    "# img = mpimg.imread(os.path.join('./preliminaryIMs/PLS', 'gg_162_0_cereb.png'))\n",
    "\n",
    "# # Output Images \n",
    "# plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bi-clustering of connectivity weights estimated using ridge regression with $\\lambda = 250$\n",
    "Results from applying spectral bi-clustering to the connectivity matrix averaged across subjects:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comments:\n",
    "\n",
    "## Model evaluation:\n",
    "\n",
    "1. For the l2regress, use log and plot in log space\n",
    "2. Think about noise ceilings: Assume there were only noise in y, then also X. What is the noise ceiling?\n",
    "3. For the subject level, what is seaborn using to plot the error bars?\n",
    "4. is the Rcv for 7 __significantly__ higher than the other ones?\n",
    "5. For both PLS and l2regress, have two extremes for the parameters. For example for PLS, start from 1 comp and go a little bit further!\n",
    "\n",
    "### Noise ceilings:\n",
    "The modeling process is a two-step process:\n",
    "First, the activations are estimated \n",
    "Second, the activity profiles for cortical parcels are used as predictors (features/regressors) to predict cerebellar activity profiles.\n",
    "\n",
    "Assuming that the estimated activations are true activations, what is the best performance we can get?\n",
    "\n",
    "The noise ceilings can be calculated at two levels:\n",
    "1. at the subject level, for models estimated for each subject: at the individual level, we have runs. We know that the model estimated for each run will be different from a model estimated from another run. To find the high noise ceiling, answer this question: assuming that the \"correct\" model (the model that gives rise to the data) is used, what is limiting the performance of the model? Then to calculate the higher and lower noise ceiling for each subject, you can take on an approach similar to the approach used for calculating the noise ceilings at the group level (see below). Besides, you can use the two sessions of data for testing separately (two different test sets) and then calculate the correlation between the predictions from the two sets ( __split-half noise ceiling__ ). \n",
    "2. at the group level:\n",
    "    * __Highe noise ceiling__: For each subject, the correlation is calculated between the weights estimated for that subject with the average of all the subjects. The final noise ceiling will then be the calculated correlations averaged over all the subjects. No model can outperform this noise ceiling model.\n",
    "    * __Low noise ceiling__: which shows how similar ( _consistant_ ) the estimated connectivity weights are across subjects. it can be the correlation between each subject's connectivity weights with all the other subjects! The overal lower noise ceiling is then the average of the correlation values across subjects.\n",
    "        * a low lower noise ceiling (near to zero or less than zero) would mean that there is essentially no similarity between the subjects\n",
    "        * a high lower noise ceiling indicates that connectivity weights are similar across subjects. \n",
    "        * if a model outperforms the lower noise ceiling, then on average, each subject's connectivity weights are more similar to the model than to the average connectivity weight of all the other subjects. (the model is better than just the average)\n",
    "            \n",
    "    \n",
    "## Component Loading maps for PLS:\n",
    "\n",
    "1. For PLS, plot comp1 vs comp2 or some other component! Might help in interpretation.\n",
    "2. thnk of different ways of interpreting the component loadings\n",
    "3. for PLS 7, it seems that the first component is just the average activation across all the tasks.\n",
    "4. for PLS 7, there seem to be a laterality in the cortical and cerebellar maps (more in the cerebellar maps) and this laterality seem to be visible in higher ordered components (the components that are explaining less covariance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}