{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling Cortico-Cerebellar Connectivity\n",
    "\n",
    "## Questions:\n",
    "* What is the best model of cortico-cerebellar connectivity? \n",
    "* What is the topography of connections between the cortex and the cerebellum?\n",
    "    * One-to-one connection topography?\n",
    "\n",
    "## Goals:\n",
    "* Estimate a model on cortical activity patterns and predict cerebellar activity pattern on a new set of data.\n",
    "* Find the most useful model of cortico-cerebellar connectivity\n",
    "\n",
    "## Background:\n",
    "### Cortico-cerebellar connectivity in non-human primates\n",
    "* (Kelly & Strick, 2003)\n",
    "\n",
    "### Cortico-cerebellar connectivity in humans\n",
    "* Methods using resting state functional connectivity:\n",
    "    * (Habas et al., 2009)\n",
    "    * (Buckner et al., 2011)\n",
    "    * (Marek et al., 2018)\n",
    "* Methods using functional gradients:\n",
    "    * (Tian et al., 2020)\n",
    "    * (Guell et al., 2018)\n",
    "\n",
    "### Gap:\n",
    "* assuming one to one connections, the previous models of cortico-cerebellar connectivity in humans, have used a winner-take-all approach and discovered the functional organization of cerebellum. __How am I going to address this using PLS or bi-clustering?__\n",
    "* cortico-cerebellar networks not assuming the one-to-one connection (with no assumptions at al)\n",
    "    \n",
    "### Hypothesis:\n",
    "* relaxing the assumption of one-to-one connection, a connectivity model that incorporates the connection topography/organization between the two structures will perform better at prediction activity patterns of the cerebellum on new dataset. _might be good if I use some materials from introduction in the bi-clustering paper_\n",
    "\n",
    "## Material and Methods:\n",
    "* MDTB dataset:\n",
    "    * two task sets, 4 sessions, 32 runs in total\n",
    "* Cortical activity pattterns were used as predictors and cerebellar activity patterns were used as responses.\n",
    "\n",
    "Re-iterating the __goal__ here: __estimate connectivity weights and connectivity pattern between the cerebellum and cortex that makes the best predictions for cerebellar activity__\n",
    "\n",
    "** How can I investigate connection topography??\n",
    "\n",
    "### Analysis:\n",
    "Multivariate multiple regression will be used to model cerebellar activity profiles as a linear function of cortical activity profiles across a range of conditions! The regression model here is:\n",
    "\\begin{align*}\n",
    "&Y_{n*m}\\ = X_{n*p}W_{p*m} + E_{n*m}\n",
    "\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Where $n$ is the number of conditions, $m$ is the number of cerebellar voxels, $p$ is the number of cortical voxels/tessels, $W$ is the matrix of connectivity weights, and $E$ is the matrix containing the residuals. $Y$ is the matrix where each column contains the activity profile of a voxel in the cerebellum (response matrix) and $X$ is the matrix where each column contains activity profile of a voxel in the neocortex (explanatory matrix, design matrix). \n",
    "\n",
    "### Limitations:\n",
    "There is high multicollinearity between explanatory variables (activity profiles of cerebellar voxels) which results from correlations between activity profiles of cortical tessles and the fact that the number of observations (in this case the number of conditions) is lower than the number of explanatory variables. This multicollinearity makes the estimated regression coefficients highly variable and will lead to poor predictions.\n",
    "\n",
    "### Regression models:\n",
    "There are a number of approaches that try to overcome this limitation. These approaches are:\n",
    "#### Step-wise Regression\n",
    "Selecting a subset of explanatory variables through a stepwise regression. This subset selection method ultimately leads to a subset of explanatory variables that best explain the variance in the response variable. This method is time-consuming for the purpose of this analysis. However, it might be a good approach to find the overlaps between cerebellar targets of cortical regions. Basically, for each cerebellar voxel, we try to find the best set of explanatory variables (cortical tessels) that best predict the activity profile, create a map of prediction accuracy of models and investigate the overlap between different models (different subsets of tassels' activity profiles)\n",
    "#### Ridge Regression\n",
    "a shrinkage method that shrinks the regression coefficients by imposing a size penalty. \n",
    "* Ridge regression and PCA: \n",
    "    * Ridge regression shrinks the direction of small variances in the design matrix the most,\n",
    "* Ridge regression as an optimization problem: It tries to optimize penalized sums of squares:\n",
    "\\begin{align*}\n",
    "&J\\ = \\sum (y_i - W_0 + \\sum x_{ij} W_j)^2 + \\lambda \\sum W_j^2\n",
    "\\\\\n",
    "\\end{align*}\n",
    "In matrix notation:\n",
    "\\begin{align*}\n",
    "&J\\ = (Y - XW)^T (Y - XW) + W^T W\n",
    "\\\\\n",
    "\\end{align*}\n",
    "    * $\\lambda$ is a parameter that determines the amount of shrinkage of the parameters. The higher the , more shrinkage will be applied\n",
    "    \n",
    "#### Principal Component Regression (PCR)\n",
    "This method tries to find a set of orthogonal variables that best explain the covariance between the explanatory variables by applying PCA to the design matrix X. Then it regresses Y on the set of these orthogonal variables. It is a two-step process and as a optimization problem: \n",
    "1. PCA to X and \n",
    "    * $X = ZV$ subject to constraint $VV^T = 1$\n",
    "        - $Z$ contains score variables and $V$ is the matrix of weights that transforms $X$ into the latent space.\n",
    "        - The objective function for this step is the objective function of PCA:\n",
    "        \\begin{align*}\n",
    "        &J\\ = |X - ZV^T|^2 = |X - XVV^T|^2\n",
    "        \\\\\n",
    "        \\end{align*}\n",
    "2. Regress $Y$ onto principal components of $X$\n",
    "    * $Y = ZW + E$ with the objective function:\n",
    "    \\begin{align*}\n",
    "    &J\\ = |Y - ZW|^2 \n",
    "    \\\\\n",
    "    \\end{align*}\n",
    "    \n",
    "Or we combine these steps together in a single step (this will give different results):\n",
    "J = Y - ZW2 = Y - XVW2, subject to constraint: VTV= 1\n",
    "\\begin{align*}\n",
    "&J\\ = |Y - ZW|^2 = |Y - XVW|^2, \\text{subject to constraint:}\\ VV^T = 1\n",
    "\\\\\n",
    "\\end{align*}\n",
    "\n",
    "As the variables are orthogonal, the problem of multicollinearity is solved. However, like all the other methods that use PCA, a question remains: What is the best number of components? In addition to this, the components are chosen to maximize the explained variance of X, not Y. Hence, the chosen components might not be good candidates to explain the highest variance in Y. To overcome the later limitation of this method, partial least squares regression (PLS-R) is used.\n",
    "\n",
    "#### Partial Least Squares Regression (PLS-R)\n",
    "This method also tries to solve the problem of multicollinearity in the design matrix. It estimates the latent space for both $X$ and $Y$ so as to maximize the covariance between the two. Basically, it tries to project both $X$ and $Y$ onto latent spaces and it tries to maximize the covariance between the latent spaces of $X$ and $Y$.\n",
    "\n",
    "\\begin{align*}\n",
    "&X_{n*p} = Z_{n*q} V_{p*q}^T + E_{n*p}\n",
    "\\\\\n",
    "\\\\\n",
    "&Y_{n*m} = U_{n*q} Q_{m*q}^T + F_{n*m}\n",
    "\\\\\n",
    "\\end{align*}\n",
    "Where $n$ is the number of conditions, $m$ is the number of cerebellar voxels, $p$ is the number of cortical voxels/tessels and $q$ is the number of PLS components.\n",
    "As an optimization problem:\n",
    "\n",
    "* As an optimization problem :\n",
    "    \\begin{align*}\n",
    "    &Z_{n*q} = X_{n*p}V_{p*q}, \\text{subject to constraint:}\\ VV^T = 1\n",
    "    \\\\\n",
    "    \\end{align*}\n",
    "    \n",
    "    $V$ is the matrix of weights that transforms $X$ into a latent space ($Z$)\n",
    "    \n",
    "    \\begin{align*}\n",
    "    &U_{n*q} = Y_{n*m}C_{m*q}, \\text{subject to constraint:}\\ CC^T = 1\n",
    "    \\\\\n",
    "    \\end{align*}\n",
    "    \n",
    "    $C$ is the matrix of weights that transforms $Y$ into a latent space ($U$)\n",
    "    \n",
    "    The objective is to find the latent variables (spaces) between which the covariance is maximized and at the same time the method tries to predict $Y$:\n",
    "    \n",
    "    \\begin{align*}\n",
    "    &cov(Z, U) = Z^TU = V^TX^TYC\n",
    "    \\\\\n",
    "    \\end{align*}\n",
    "    \n",
    "    \n",
    "* There are different implementations of the PLSR, each with different assumptions. There are both __iterative__ and __non-iterative__ methods. To my understanding the iterative method is the most common one. But there is also a non-iterative method. The difference between the results of iterative and non-iterative methods is in the __orthogonality__ of final latent variables. Briefly, using the iterative methods like __PLS1__ (for 1-D response variable), __PLS2__, and __SIMPLS__, the latent variables will be mutually orthogonal as these methods are __iterative__ and in the end of each iteration, the effect of each estimated latent variable is subtracted from the original matrices, a process called __deflation__. The deflation process guarantees mutual orthogonality of latent variables. There is, however, a non-iterative approach. The latent variables estimated using this method will not, in general, be mutually orthogonal. In any case, there is a eigenvalue problem. The iterative methods try to solve the eigenvalue problem iteratively and the non-iterative method tries to solve this eigenvalue problem in one go.\n",
    "* The algorithm tries to find $T$ and $U$ __iteratively__ and then calculates the regression coefficients as $b_i= z_i^T u_i$ at each iteration of the algorithm. Eventually, we will have a diagonal matrix ($B$) where each element is calculated in each iteration. Finally:\n",
    "\n",
    "    \\begin{align*}\n",
    "    &Y_{n*m} = X_{n*p}W_{pls}\n",
    "    \\\\\n",
    "    \\end{align*}\n",
    "    \n",
    "    where $W_{pls}= V_{p*q}^T + B_{q*q} C_{q*m}^T$. Based on this formulation, $W_{pls}$ is a $p*m$ matrix\n",
    "    \n",
    "    \n",
    "* _I need to clarify something here to better understand how the dimensions of the matrices are determined:_\n",
    "    For $A_{p*q}$, The pseudoinverse of $A$ is defined as $A_{q*p}^+$. This is useful in determining the dimensions of matrix $V$ and $W_{pls}$.\n",
    "    \n",
    "* _Keep in mind that we have $V_{p*q}$, $V_{q*p}^T$, and based on the relationship between a matrix and its pseudoinverse: $V_{p*q}^{T+}$_ \n",
    "* $B_{q*q}$ is a diagonal matrix.\n",
    "    * __Hypothesis__: _In this model, $B$ being diagonal implies that the one-to-one topography is valid. However, not restricting B to be diagonal will imply that the one-to-one topography does not necessarily hold. My hypothesis is that not restricting B to be a diagonal matrix, in other words not imposing the one-to-one connection topography, will yield better predictions. __What would this mean in terms of mathematical implementation of the algorithm to estimate the parameters and latent structures?___\n",
    "\n",
    "#### Simultaneous Parameter Learning and Bi-clustering for Multi-Response Models (Yu et al., 2019)\n",
    "\n",
    "\n",
    "## Results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages needed to visualize the results\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import evaluation  # user-defined package\n",
    "import os\n",
    "\n",
    "\n",
    "import glob\n",
    "import nibabel as nib\n",
    "\n",
    "from nilearn import plotting\n",
    "from nilearn import surface\n",
    "from nilearn import datasets\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ipywidgets as widgets       # interactive display\n",
    "%config InlineBackend.figure_format = 'svg' # other available formats are: 'retina', 'png', 'jpeg', 'pdf'\n",
    "# plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/nma.mplstyle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To visualize the results:\n",
    "# sestting defaults for some variables\n",
    "returnSubjs = np.array([2,3,4,6,8,9,10,12,14,15,17,18,19,20,21,22,24,25,26,27,28,29,30,31])\n",
    "# Setting different options of the functions\n",
    "roi_dict1        = {'cortex':'tesselsWB162', 'cerebellum':'grey_nan'} \n",
    "roi_dict2        = {'cortex':'tesselsWB362', 'cerebellum':'grey_nan'} \n",
    "roi_dict3        = {'cortex':'tesselsWB642', 'cerebellum':'grey_nan'} \n",
    "trainExperiment  = 1\n",
    "tMode            = 'crossed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with all the evaluation info\n",
    "DF = evaluation.eval_df(sn = returnSubjs, glm = 7, models = ['l2regress', 'plsregress'], \n",
    "                 rois = ['tesselsWB162', 'tesselsWB362', 'tesselsWB642'], \n",
    "                 testExper = 2)\n",
    "\n",
    "# save the dataframe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb5ee5dc8f8242ef8e62489ae8eec8bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='model', options=('plsregress', 'l2regress'), value='plsregress'), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# interactive visualization of the results\n",
    "@widgets.interact(model = ['plsregress', 'l2regress'],\n",
    "                  xname = ['162', '362', '642'], \n",
    "                  yvar  = ['Rcv', 'Rnc', 'Rp'])\n",
    "def plot_model_params(model, xname, yvar):\n",
    "    # gets the section of the dataframe corresponding to the model\n",
    "    dfx = DF.loc[DF.xname == 'tesselsWB%s'%xname]\n",
    "    dfx_model = dfx.loc[dfx.model == model]\n",
    "    \n",
    "    # sort the dataframe based on values of params\n",
    "    df2 = dfx_model.sort_values(by ='params' , ascending=False)\n",
    "    \n",
    "    # plot\n",
    "    figure, axes = plt. subplots(nrows=1, ncols=2)\n",
    "    plt.subplot(121)\n",
    "    sns.barplot(x='model', y=yvar, hue='params', data=df2)\n",
    "    plt.subplot(122)\n",
    "    sns.lineplot(x='params', y=yvar, marker='o', err_style = 'band', \n",
    "                 dashes = False, data=df2)\n",
    "    figure. tight_layout(pad=3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize PLS component loading maps for 7-component model\n",
    "class Defaults: \n",
    "    BASE_DIR = '/Users/ladan/Documents/MATLAB/Projects/Connectivity/Connectivity_MDTB/python_code'\n",
    "    CONN_DIR = os.path.join(BASE_DIR, 'sc1', 'connModels')\n",
    "    NCOMP    = 7 # number of components\n",
    "    SUIT_FUNCTIONAL_DIR = os.path.join(BASE_DIR, \"preliminaryIMs\")\n",
    "    SUIT_ANATOMICAL_DIR = '/Users/ladan/Documents/MATLAB/suit/flatmap'\n",
    "    \n",
    "    \n",
    "class VisualizeCerebellum:\n",
    "    def __init__(self):\n",
    "        self.contrast_type = \"group_PLS\"\n",
    "        self.glm = \"glm7\" \n",
    "        self.vmax = 0.003\n",
    "        self.surf_mesh = os.path.join(Defaults.SUIT_ANATOMICAL_DIR, \"FLAT.surf.gii\")\n",
    "\n",
    "    def plot_surface(self, comp_numb):\n",
    "        view = plotting.view_surf(surf_mesh = self.surf_mesh, \n",
    "                                surf_map    = self.surf_map, \n",
    "                                colorbar    = True,\n",
    "                                vmax        = self.vmax,\n",
    "                                title       = 'comp%d'%comp_numb) \n",
    "        view.open_in_browser()\n",
    "   \n",
    "    def visualize_loading_surface(self, comps_numb, num, xres):\n",
    "\n",
    "        # get functional group dir\n",
    "\n",
    "        # get all contrast images in gifti format\n",
    "        fpath = os.path.join(Defaults.BASE_DIR,\n",
    "                             Defaults.SUIT_FUNCTIONAL_DIR,\n",
    "                             f\"{self.contrast_type}{comps_numb}_{num}_{xres}.func.gii\")\n",
    "\n",
    "        # get surface mesh for SUIT\n",
    "        self.surf_map = surface.load_surf_data(fpath).astype(int)\n",
    "        self.surf_mesh = os.path.join(Defaults.SUIT_ANATOMICAL_DIR, \"FLAT.surf.gii\")\n",
    "\n",
    "        self.plot_surface(num) \n",
    "\n",
    "\n",
    "# # example code to visualize group contrast(s) on flat map\n",
    "vis = VisualizeCerebellum()\n",
    "vis.visualize_loading_surface(7, 3, 162)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
